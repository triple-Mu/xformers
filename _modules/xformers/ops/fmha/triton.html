


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
<!--<![endif]-->

<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>xformers.ops.fmha.triton | xFormers 0.0.25 documentation</title>
  
  <script src="../../../../_static/js/ga.js"></script>
  <script src="../../../../_static/js/redirect.js"></script>
  
  <link rel="shortcut icon" href="../../../../_static/images/favicon.png" />
  
  
  <link rel="canonical" href="https://facebookresearch.github.io/xformers_modules/xformers/ops/fmha/triton.html" />
  
  <meta property="og:title" content="xformers.ops.fmha.triton | xFormers 0.0.25 documentation">
  <meta name="description" content="API docs for xFormers. xFormers is a PyTorch extension library for composable and optimized Transformer blocks.">
  <meta property="og:description" content="API docs for xFormers. xFormers is a PyTorch extension library for composable and optimized Transformer blocks.">
  <!--<meta property="og:image" content="https://mmf.sh/img/logo.png">-->
  <!--<meta property="twitter:image" content="https://mmf.sh/img/logo.png">-->
  <meta name="twitter:image:alt" content="Image for xFormers">
  <meta name="twitter:card" content="summary_large_image">
  

  
  
  

  

  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../../_static/css/customize.css" type="text/css" />
  <link rel="index" title="Index" href="../../../../genindex.html" />
  <link rel="search" title="Search" href="../../../../search.html" /> 

  
  <script src="../../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://github.com/facebookresearch/xformers"><img
          src="../../../../_static/logo.png" style="padding-right: 90px;" width="280px" height="53px"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/facebookresearch/xformers"> xFormers Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>

  </div>
</div>


<body class="pytorch-body">

   

  

  <div class="table-of-contents-link-wrapper">
    <span>Table of Contents</span>
    <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
  </div>

  <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
    <div class="pytorch-side-scroll">
      <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        <div class="pytorch-left-menu-search">
          

          
          
          
          

          


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        
        
        
        
        
        <p class="caption" role="heading"><span class="caption-text">Index</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../what_is_xformers.html">What is xFormers?</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Components Documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../components/index.html">API Reference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials and examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/index.html">Tutorials</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Some custom parts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../custom_parts/index.html">Custom parts reference</a></li>
</ul>

        
        
      </div>
    </div>
  </nav>

  <div class="pytorch-container">
    <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
      <div class="pytorch-breadcrumbs-wrapper">
        















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../fmha.html">xformers.ops.fmha</a> &gt;</li>
        
      <li>xformers.ops.fmha.triton</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
      </div>

      <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
        Shortcuts
      </div>
    </div>

    <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
      <div class="pytorch-content-left">

        
        
          <div class="rst-content">
            
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
              <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
                
  <h1>Source code for xformers.ops.fmha.triton</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># This source code is licensed under the BSD license found in the</span>
<span class="c1"># LICENSE file in the root directory of this source tree.</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Triton Flash Attention 2</span>
<span class="sd">Based on</span>
<span class="sd">https://github.com/openai/triton/blob/293b7fd592a1602f2305c1bd0bc978bbd97337d6/python/tutorials/06-fused-attention.py  # noqa: E501</span>
<span class="sd">https://github.com/openai/triton/blob/293b7fd592a1602f2305c1bd0bc978bbd97337d6/python/triton/ops/flash_attention.py  # noqa: E501</span>
<span class="sd">https://github.com/Dao-AILab/flash-attention/blob/dd9a6fa45a9b90ff954d2b3f3f44241b9216190e/flash_attn/flash_attn_triton.py  # noqa: E501</span>
<span class="sd">https://github.com/ROCmSoftwarePlatform/triton/blob/670ae8054da008424097989a5b6e3816aa601e07/python/perf-kernels/06-fused-attention-transV.py  # noqa: E501</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">replace</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Mapping</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Set</span><span class="p">,</span> <span class="n">Tuple</span>

<span class="kn">import</span> <span class="nn">torch</span>

<span class="kn">from</span> <span class="nn">xformers</span> <span class="kn">import</span> <span class="n">_is_triton_available</span>

<span class="kn">from</span> <span class="nn">..common</span> <span class="kn">import</span> <span class="n">register_operator</span>
<span class="kn">from</span> <span class="nn">.attn_bias</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">BlockDiagonalCausalMask</span><span class="p">,</span>
    <span class="n">BlockDiagonalCausalWithOffsetPaddedKeysMask</span><span class="p">,</span>
    <span class="n">LowerTriangularMask</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">.common</span> <span class="kn">import</span> <span class="n">AttentionFwOpBase</span><span class="p">,</span> <span class="n">Context</span><span class="p">,</span> <span class="n">Inputs</span><span class="p">,</span> <span class="n">check_lastdim_alignment_stride1</span>

<span class="k">if</span> <span class="n">_is_triton_available</span><span class="p">():</span>
    <span class="kn">import</span> <span class="nn">triton</span>
    <span class="kn">import</span> <span class="nn">triton.language</span> <span class="k">as</span> <span class="nn">tl</span>

    <span class="nd">@triton</span><span class="o">.</span><span class="n">jit</span>
    <span class="k">def</span> <span class="nf">_fwd_kernel_triton_flash_inner</span><span class="p">(</span>
        <span class="n">acc</span><span class="p">,</span>
        <span class="n">l_i</span><span class="p">,</span>
        <span class="n">m_i</span><span class="p">,</span>
        <span class="n">q</span><span class="p">,</span>
        <span class="n">K_block_ptr</span><span class="p">,</span>
        <span class="n">V_block_ptr</span><span class="p">,</span>
        <span class="n">q_seq_start</span><span class="p">,</span>
        <span class="n">lo</span><span class="p">,</span>
        <span class="n">hi</span><span class="p">,</span>
        <span class="n">start_m</span><span class="p">,</span>
        <span class="n">qk_scale</span><span class="p">,</span>
        <span class="n">kv_len</span><span class="p">,</span>
        <span class="n">offs_m</span><span class="p">,</span>
        <span class="n">offs_n</span><span class="p">,</span>
        <span class="n">BLOCK_M</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span>
        <span class="n">BLOCK_N</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span>
        <span class="n">IS_CAUSAL</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span>
        <span class="n">BOUNDS_CHECKS_N</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span>
        <span class="n">CAST_BEFORE_MATMUL</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span>
        <span class="n">ALLOW_TF32</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span>
        <span class="n">STAGE</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span>
        <span class="n">pre_load_v</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">BOUNDS_CHECKS_STAGE</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span> <span class="o">=</span> <span class="n">BOUNDS_CHECKS_N</span> <span class="ow">and</span> <span class="n">STAGE</span> <span class="o">==</span> <span class="mi">2</span>
        <span class="c1"># Doesn&#39;t seem to make a difference</span>
        <span class="k">if</span> <span class="n">STAGE</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">lo</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">lo</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">multiple_of</span><span class="p">(</span><span class="n">lo</span><span class="p">,</span> <span class="n">BLOCK_N</span><span class="p">)</span>
            <span class="n">K_block_ptr</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">advance</span><span class="p">(</span><span class="n">K_block_ptr</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">lo</span><span class="p">))</span>
            <span class="n">V_block_ptr</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">advance</span><span class="p">(</span><span class="n">V_block_ptr</span><span class="p">,</span> <span class="p">(</span><span class="n">lo</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>

        <span class="c1"># loop over k, v and update accumulator</span>
        <span class="k">for</span> <span class="n">start_n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">lo</span><span class="p">,</span> <span class="n">hi</span><span class="p">,</span> <span class="n">BLOCK_N</span><span class="p">):</span>
            <span class="n">start_n</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">multiple_of</span><span class="p">(</span>
                <span class="n">start_n</span><span class="p">,</span> <span class="n">BLOCK_N</span>
            <span class="p">)</span>  <span class="c1"># doesn&#39;t seem to make a difference</span>
            <span class="c1"># -- load k, v --</span>
            <span class="n">k</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">K_block_ptr</span><span class="p">,</span> <span class="n">boundary_check</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="k">if</span> <span class="n">BOUNDS_CHECKS_STAGE</span> <span class="k">else</span> <span class="p">())</span>
            <span class="c1"># Moving masking here seems to introduce num errors,</span>
            <span class="c1"># e.g. in test_forward[tritonflashattF-cuda-torch.bfloat16-NoneType-1-256-15-1-32-32-False-BMHK]</span>
            <span class="c1"># if BOUNDS_CHECKS_N or USE_SEQ_LEN:</span>
            <span class="c1">#     k = tl.where(hi - tl.arange(0, BLOCK_N) &gt; start_n, k, float(&quot;-inf&quot;))</span>
            <span class="k">if</span> <span class="n">pre_load_v</span><span class="p">:</span>
                <span class="n">v</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span>
                    <span class="n">V_block_ptr</span><span class="p">,</span> <span class="n">boundary_check</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="k">if</span> <span class="n">BOUNDS_CHECKS_STAGE</span> <span class="k">else</span> <span class="p">()</span>
                <span class="p">)</span>
            <span class="c1"># -- compute qk ---</span>
            <span class="n">qk</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">BLOCK_M</span><span class="p">,</span> <span class="n">BLOCK_N</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tl</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
            <span class="n">qk</span> <span class="o">+=</span> <span class="n">tl</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">k</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">k</span><span class="p">,</span> <span class="n">allow_tf32</span><span class="o">=</span><span class="n">ALLOW_TF32</span><span class="p">)</span> <span class="o">*</span> <span class="n">qk_scale</span>
            <span class="k">if</span> <span class="n">CAST_BEFORE_MATMUL</span><span class="p">:</span>
                <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">tl</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">STAGE</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">IS_CAUSAL</span><span class="p">:</span>
                    <span class="c1"># For some reason this is faster than start_n &lt;= q_seq_start + offs_m[:, None] - offs_n[None, :]</span>
                    <span class="n">qk</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
                        <span class="n">q_seq_start</span> <span class="o">+</span> <span class="n">offs_m</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="n">start_n</span> <span class="o">+</span> <span class="n">offs_n</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]),</span>
                        <span class="n">qk</span><span class="p">,</span>
                        <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">),</span>
                    <span class="p">)</span>
                <span class="k">if</span> <span class="n">BOUNDS_CHECKS_N</span><span class="p">:</span>
                    <span class="n">qk</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
                        <span class="n">tl</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_N</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">hi</span> <span class="o">-</span> <span class="n">start_n</span><span class="p">,</span> <span class="n">qk</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">)</span>
                    <span class="p">)</span>

            <span class="c1"># -- compute scaling constant ---</span>
            <span class="n">m_i_new</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">m_i</span><span class="p">,</span> <span class="n">tl</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">qk</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
            <span class="n">qk</span> <span class="o">=</span> <span class="n">qk</span> <span class="o">-</span> <span class="n">m_i_new</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>
            <span class="n">alpha</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">exp2</span><span class="p">(</span><span class="n">m_i</span> <span class="o">-</span> <span class="n">m_i_new</span><span class="p">)</span>
            <span class="n">p</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">exp2</span><span class="p">(</span><span class="n">qk</span><span class="p">)</span>

            <span class="c1"># -- scale and update acc --</span>
            <span class="n">acc</span> <span class="o">*=</span> <span class="n">alpha</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">pre_load_v</span><span class="p">:</span>
                <span class="n">v</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span>
                    <span class="n">V_block_ptr</span><span class="p">,</span> <span class="n">boundary_check</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="k">if</span> <span class="n">BOUNDS_CHECKS_STAGE</span> <span class="k">else</span> <span class="p">()</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">CAST_BEFORE_MATMUL</span><span class="p">:</span>
                <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">tl</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
            <span class="n">acc</span> <span class="o">+=</span> <span class="n">tl</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">v</span><span class="p">,</span> <span class="n">allow_tf32</span><span class="o">=</span><span class="n">ALLOW_TF32</span><span class="p">)</span>
            <span class="c1"># -- update m_i and l_i --</span>
            <span class="n">l_i</span> <span class="o">=</span> <span class="n">l_i</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">+</span> <span class="n">tl</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">m_i</span> <span class="o">=</span> <span class="n">m_i_new</span>
            <span class="c1"># update pointers</span>
            <span class="n">K_block_ptr</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">advance</span><span class="p">(</span><span class="n">K_block_ptr</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_N</span><span class="p">))</span>
            <span class="n">V_block_ptr</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">advance</span><span class="p">(</span><span class="n">V_block_ptr</span><span class="p">,</span> <span class="p">(</span><span class="n">BLOCK_N</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">acc</span><span class="p">,</span> <span class="n">l_i</span><span class="p">,</span> <span class="n">m_i</span>

    <span class="nd">@triton</span><span class="o">.</span><span class="n">jit</span>
    <span class="k">def</span> <span class="nf">_fwd_kernel_triton_flash</span><span class="p">(</span>
        <span class="n">Q</span><span class="p">,</span>
        <span class="n">K</span><span class="p">,</span>
        <span class="n">V</span><span class="p">,</span>
        <span class="n">sm_scale</span><span class="p">,</span>
        <span class="n">L</span><span class="p">,</span>
        <span class="n">Out</span><span class="p">,</span>
        <span class="n">Seq_len</span><span class="p">,</span>
        <span class="n">Seq_pos_q</span><span class="p">,</span>
        <span class="n">stride_qz</span><span class="p">,</span>
        <span class="n">stride_qh</span><span class="p">,</span>
        <span class="n">stride_qm</span><span class="p">,</span>
        <span class="n">stride_qk</span><span class="p">,</span>
        <span class="n">stride_kz</span><span class="p">,</span>
        <span class="n">stride_kh</span><span class="p">,</span>
        <span class="n">stride_kn</span><span class="p">,</span>
        <span class="n">stride_kk</span><span class="p">,</span>
        <span class="n">stride_vz</span><span class="p">,</span>
        <span class="n">stride_vh</span><span class="p">,</span>
        <span class="n">stride_vk</span><span class="p">,</span>
        <span class="n">stride_vn</span><span class="p">,</span>
        <span class="n">stride_oz</span><span class="p">,</span>
        <span class="n">stride_oh</span><span class="p">,</span>
        <span class="n">stride_om</span><span class="p">,</span>
        <span class="n">stride_on</span><span class="p">,</span>
        <span class="n">Z</span><span class="p">,</span>
        <span class="n">H</span><span class="p">,</span>
        <span class="n">N_CTX</span><span class="p">,</span>
        <span class="n">Mkv</span><span class="p">,</span>
        <span class="n">BLOCK_M</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span>
        <span class="n">BLOCK_DMODEL</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span>
        <span class="n">BLOCK_N</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span>
        <span class="n">IS_CAUSAL</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span>
        <span class="n">BOUNDS_CHECKS_N</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span>
        <span class="n">BOUNDS_CHECKS_M</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span>
        <span class="n">ALLOW_TF32</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span>
        <span class="n">CAST_BEFORE_MATMUL</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span>
        <span class="n">USE_SEQ_LEN_KV</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span>
        <span class="n">USE_SEQ_POS_Q</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span>
        <span class="n">IS_KV_PADDED</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span>  <span class="c1"># Switch between padded and non-padded block-diagonal causal masks</span>
        <span class="n">pre_load_v</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span>  <span class="c1"># TODO: understand if that matters</span>
    <span class="p">):</span>
        <span class="n">start_m</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">program_id</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">tl</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="n">off_hz</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">program_id</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">tl</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>

        <span class="n">tl</span><span class="o">.</span><span class="n">static_assert</span><span class="p">((</span><span class="n">IS_KV_PADDED</span> <span class="ow">and</span> <span class="n">USE_SEQ_POS_Q</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">IS_KV_PADDED</span><span class="p">)</span>

        <span class="n">off_z</span> <span class="o">=</span> <span class="n">off_hz</span> <span class="o">//</span> <span class="n">H</span>
        <span class="n">off_h</span> <span class="o">=</span> <span class="n">off_hz</span> <span class="o">%</span> <span class="n">H</span>
        <span class="k">if</span> <span class="n">USE_SEQ_POS_Q</span><span class="p">:</span>
            <span class="n">seqpos</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">Seq_pos_q</span> <span class="o">+</span> <span class="n">off_z</span><span class="p">)</span>
            <span class="n">seqpos_next</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">Seq_pos_q</span> <span class="o">+</span> <span class="n">off_z</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">q_len</span> <span class="o">=</span> <span class="n">seqpos_next</span> <span class="o">-</span> <span class="n">seqpos</span>
            <span class="n">q_offset</span> <span class="o">=</span> <span class="n">seqpos</span> <span class="o">*</span> <span class="n">stride_qm</span> <span class="o">+</span> <span class="n">off_h</span> <span class="o">*</span> <span class="n">stride_qh</span>
            <span class="n">out_offset</span> <span class="o">=</span> <span class="n">seqpos</span> <span class="o">*</span> <span class="n">stride_om</span> <span class="o">+</span> <span class="n">off_h</span> <span class="o">*</span> <span class="n">stride_oh</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">IS_KV_PADDED</span><span class="p">:</span>
                <span class="c1"># BlockDiagonalCausalMask, no padding, use same sequence positions as for Q</span>
                <span class="n">kv_offset</span> <span class="o">=</span> <span class="n">seqpos</span> <span class="o">*</span> <span class="n">stride_kn</span> <span class="o">+</span> <span class="n">off_h</span> <span class="o">*</span> <span class="n">stride_kh</span>
                <span class="n">kv_len</span> <span class="o">=</span> <span class="n">q_len</span>
                <span class="n">q_seq_start</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># BlockDiagonalCausalWithOffsetPaddedKeysMask</span>
                <span class="n">kv_offset</span> <span class="o">=</span> <span class="n">off_z</span> <span class="o">*</span> <span class="n">stride_kz</span> <span class="o">+</span> <span class="n">off_h</span> <span class="o">*</span> <span class="n">stride_kh</span>
                <span class="k">if</span> <span class="n">USE_SEQ_LEN_KV</span><span class="p">:</span>
                    <span class="n">kv_len</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">Seq_len</span> <span class="o">+</span> <span class="n">off_z</span><span class="p">)</span>
                    <span class="n">q_seq_start</span> <span class="o">=</span> <span class="n">kv_len</span> <span class="o">-</span> <span class="n">q_len</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># if no variable K/V seqlens are provided, assume full length</span>
                    <span class="n">kv_len</span> <span class="o">=</span> <span class="n">Mkv</span>
                    <span class="n">q_seq_start</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># No mask or simple causal mask</span>
            <span class="n">q_len</span> <span class="o">=</span> <span class="n">N_CTX</span>
            <span class="n">q_offset</span> <span class="o">=</span> <span class="n">off_z</span> <span class="o">*</span> <span class="n">stride_qz</span> <span class="o">+</span> <span class="n">off_h</span> <span class="o">*</span> <span class="n">stride_qh</span>
            <span class="n">out_offset</span> <span class="o">=</span> <span class="n">off_z</span> <span class="o">*</span> <span class="n">stride_oz</span> <span class="o">+</span> <span class="n">off_h</span> <span class="o">*</span> <span class="n">stride_oh</span>

            <span class="n">kv_len</span> <span class="o">=</span> <span class="n">Mkv</span>
            <span class="n">q_seq_start</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">kv_offset</span> <span class="o">=</span> <span class="n">off_z</span> <span class="o">*</span> <span class="n">stride_kz</span> <span class="o">+</span> <span class="n">off_h</span> <span class="o">*</span> <span class="n">stride_kh</span>

        <span class="n">Q_block_ptr</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">make_block_ptr</span><span class="p">(</span>
            <span class="n">base</span><span class="o">=</span><span class="n">Q</span> <span class="o">+</span> <span class="n">q_offset</span><span class="p">,</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">q_len</span><span class="p">,</span> <span class="n">BLOCK_DMODEL</span><span class="p">),</span>
            <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="n">stride_qm</span><span class="p">,</span> <span class="n">stride_qk</span><span class="p">),</span>
            <span class="n">offsets</span><span class="o">=</span><span class="p">(</span><span class="n">start_m</span> <span class="o">*</span> <span class="n">BLOCK_M</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
            <span class="n">block_shape</span><span class="o">=</span><span class="p">(</span><span class="n">BLOCK_M</span><span class="p">,</span> <span class="n">BLOCK_DMODEL</span><span class="p">),</span>
            <span class="n">order</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="n">K_block_ptr</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">make_block_ptr</span><span class="p">(</span>
            <span class="n">base</span><span class="o">=</span><span class="n">K</span> <span class="o">+</span> <span class="n">kv_offset</span><span class="p">,</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">BLOCK_DMODEL</span><span class="p">,</span> <span class="n">kv_len</span><span class="p">),</span>
            <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="n">stride_kk</span><span class="p">,</span> <span class="n">stride_kn</span><span class="p">),</span>
            <span class="n">offsets</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
            <span class="n">block_shape</span><span class="o">=</span><span class="p">(</span><span class="n">BLOCK_DMODEL</span><span class="p">,</span> <span class="n">BLOCK_N</span><span class="p">),</span>
            <span class="n">order</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="n">V_block_ptr</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">make_block_ptr</span><span class="p">(</span>
            <span class="n">base</span><span class="o">=</span><span class="n">V</span> <span class="o">+</span> <span class="n">kv_offset</span><span class="p">,</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">kv_len</span><span class="p">,</span> <span class="n">BLOCK_DMODEL</span><span class="p">),</span>
            <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="n">stride_vk</span><span class="p">,</span> <span class="n">stride_vn</span><span class="p">),</span>
            <span class="n">offsets</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
            <span class="n">block_shape</span><span class="o">=</span><span class="p">(</span><span class="n">BLOCK_N</span><span class="p">,</span> <span class="n">BLOCK_DMODEL</span><span class="p">),</span>
            <span class="n">order</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="c1"># initialize offsets</span>
        <span class="n">offs_m</span> <span class="o">=</span> <span class="n">start_m</span> <span class="o">*</span> <span class="n">BLOCK_M</span> <span class="o">+</span> <span class="n">tl</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_M</span><span class="p">)</span>  <span class="c1"># For Q</span>
        <span class="n">offs_n</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_N</span><span class="p">)</span>  <span class="c1"># For K/V</span>
        <span class="c1"># initialize pointer to m and l</span>
        <span class="n">m_i</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">BLOCK_M</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tl</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">-</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)</span>
        <span class="n">l_i</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">BLOCK_M</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tl</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1.0</span>
        <span class="n">acc</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">BLOCK_M</span><span class="p">,</span> <span class="n">BLOCK_DMODEL</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tl</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="c1"># scale sm_scale by log_2(e) and use</span>
        <span class="c1"># 2^x instead of exp in the loop because CSE and LICM</span>
        <span class="c1"># don&#39;t work as expected with `exp` in the loop</span>
        <span class="n">qk_scale</span> <span class="o">=</span> <span class="n">sm_scale</span> <span class="o">*</span> <span class="mf">1.44269504</span>
        <span class="c1"># load q: it will stay in SRAM throughout on NV GPUs but in VGPRs on AMD GPUs</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span>
            <span class="n">Q_block_ptr</span><span class="p">,</span> <span class="n">boundary_check</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="k">if</span> <span class="n">BOUNDS_CHECKS_M</span> <span class="ow">or</span> <span class="n">USE_SEQ_POS_Q</span> <span class="k">else</span> <span class="p">()</span>
        <span class="p">)</span>

        <span class="c1"># The loop over K/V sequence blocks is divided into two stages:</span>
        <span class="c1"># Stage 1: (many) blocks which don&#39;t need boundary conditions checks - not touching sequence end or diagonal</span>
        <span class="c1"># Stage 2: (few) blocks which need boundary conditions checks</span>
        <span class="c1"># Following https://github.com/openai/triton/blob/293b7fd592a1602f2305c1bd0bc978bbd97337d6/python/tutorials/06-fused-attention.py  # noqa: E501</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Iteration doesn&#39;t need masking if</span>
<span class="sd">            - 1) block doesn&#39;t cross the diagonal: max(kv_pos) &lt;= min(q_pos)</span>
<span class="sd">            - 2) block doesn&#39;t cross the end of the sequence: max(kv_pos) &lt; kv_len</span>
<span class="sd">        Find maximum start_n for which condition 1 is satisifed.</span>
<span class="sd">        Remember that</span>
<span class="sd">            q_pos = q_seq_start + offs_m[:, None]</span>
<span class="sd">            kv_pos = start_n + offs_n[None, :]</span>
<span class="sd">            offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)</span>
<span class="sd">            offs_n = tl.arange(0, BLOCK_N)</span>
<span class="sd">        min(q_pos) = q_seq_start + start_m * BLOCK_M</span>
<span class="sd">        max(kv_pos) = start_n + BLOCK_N - 1</span>
<span class="sd">        So the condition becomes</span>
<span class="sd">            q_seq_start + start_m * BLOCK_M &gt;= start_n + BLOCK_N - 1</span>
<span class="sd">        So:</span>
<span class="sd">        1) start_n &lt;= q_seq_start + start_m * BLOCK_M - BLOCK_N + 1</span>
<span class="sd">        2) start_n &lt;= kv_len - BLOCK_N</span>

<span class="sd">        So the last allowed start_n without masking is min(q_seq_start + start_m * BLOCK_M + 1, kv_len) - BLOCK_N</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Second stage can only be skipped if no mask is used and K/V length is divisible by the tile size</span>
        <span class="n">TWO_STAGES</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span> <span class="o">=</span> <span class="n">BOUNDS_CHECKS_N</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="n">IS_CAUSAL</span> <span class="ow">or</span> <span class="p">(</span><span class="n">USE_SEQ_LEN_KV</span> <span class="ow">or</span> <span class="p">(</span><span class="n">USE_SEQ_POS_Q</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">IS_KV_PADDED</span><span class="p">))</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">TWO_STAGES</span><span class="p">:</span>
            <span class="c1"># Border between two stages</span>
            <span class="n">hi_stage_1</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">q_seq_start</span> <span class="o">+</span> <span class="n">start_m</span> <span class="o">*</span> <span class="n">BLOCK_M</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">kv_len</span><span class="p">)</span> <span class="o">-</span> <span class="n">BLOCK_N</span>
            <span class="n">hi_stage_1</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">hi_stage_1</span> <span class="o">//</span> <span class="n">BLOCK_N</span>
            <span class="p">)</span> <span class="o">*</span> <span class="n">BLOCK_N</span>  <span class="c1"># Don&#39;t understand why it doesn&#39;t work without this</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">hi_stage_1</span> <span class="o">=</span> <span class="n">kv_len</span>

        <span class="c1"># Stage 1 - no boundary conditions</span>
        <span class="n">acc</span><span class="p">,</span> <span class="n">l_i</span><span class="p">,</span> <span class="n">m_i</span> <span class="o">=</span> <span class="n">_fwd_kernel_triton_flash_inner</span><span class="p">(</span>
            <span class="n">acc</span><span class="p">,</span>
            <span class="n">l_i</span><span class="p">,</span>
            <span class="n">m_i</span><span class="p">,</span>
            <span class="n">q</span><span class="p">,</span>
            <span class="n">K_block_ptr</span><span class="p">,</span>
            <span class="n">V_block_ptr</span><span class="p">,</span>
            <span class="n">q_seq_start</span><span class="p">,</span>
            <span class="mi">0</span><span class="p">,</span>
            <span class="n">hi_stage_1</span><span class="p">,</span>
            <span class="n">start_m</span><span class="p">,</span>
            <span class="n">qk_scale</span><span class="p">,</span>
            <span class="n">kv_len</span><span class="p">,</span>
            <span class="n">offs_m</span><span class="p">,</span>
            <span class="n">offs_n</span><span class="p">,</span>
            <span class="n">BLOCK_M</span><span class="p">,</span>
            <span class="n">BLOCK_N</span><span class="p">,</span>
            <span class="n">IS_CAUSAL</span><span class="p">,</span>
            <span class="n">BOUNDS_CHECKS_N</span><span class="p">,</span>
            <span class="n">CAST_BEFORE_MATMUL</span><span class="p">,</span>
            <span class="n">ALLOW_TF32</span><span class="p">,</span>
            <span class="n">STAGE</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">pre_load_v</span><span class="o">=</span><span class="n">pre_load_v</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">TWO_STAGES</span><span class="p">:</span>
            <span class="n">hi</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">tl</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">kv_len</span><span class="p">,</span> <span class="n">q_seq_start</span> <span class="o">+</span> <span class="p">(</span><span class="n">start_m</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">BLOCK_M</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">IS_CAUSAL</span>
                <span class="k">else</span> <span class="n">kv_len</span>
            <span class="p">)</span>
            <span class="c1"># Do we need this barrier?</span>
            <span class="c1"># tl.debug_barrier()</span>
            <span class="c1"># Stage 2 - with boundary conditions</span>
            <span class="n">acc</span><span class="p">,</span> <span class="n">l_i</span><span class="p">,</span> <span class="n">m_i</span> <span class="o">=</span> <span class="n">_fwd_kernel_triton_flash_inner</span><span class="p">(</span>
                <span class="n">acc</span><span class="p">,</span>
                <span class="n">l_i</span><span class="p">,</span>
                <span class="n">m_i</span><span class="p">,</span>
                <span class="n">q</span><span class="p">,</span>
                <span class="n">K_block_ptr</span><span class="p">,</span>
                <span class="n">V_block_ptr</span><span class="p">,</span>
                <span class="n">q_seq_start</span><span class="p">,</span>
                <span class="n">hi_stage_1</span><span class="p">,</span>
                <span class="n">hi</span><span class="p">,</span>
                <span class="n">start_m</span><span class="p">,</span>
                <span class="n">qk_scale</span><span class="p">,</span>
                <span class="n">kv_len</span><span class="p">,</span>
                <span class="n">offs_m</span><span class="p">,</span>
                <span class="n">offs_n</span><span class="p">,</span>
                <span class="n">BLOCK_M</span><span class="p">,</span>
                <span class="n">BLOCK_N</span><span class="p">,</span>
                <span class="n">IS_CAUSAL</span><span class="p">,</span>
                <span class="n">BOUNDS_CHECKS_N</span><span class="p">,</span>
                <span class="n">CAST_BEFORE_MATMUL</span><span class="p">,</span>
                <span class="n">ALLOW_TF32</span><span class="p">,</span>
                <span class="n">STAGE</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                <span class="n">pre_load_v</span><span class="o">=</span><span class="n">pre_load_v</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># write back l and m</span>
        <span class="n">acc1</span> <span class="o">=</span> <span class="n">acc</span> <span class="o">/</span> <span class="n">l_i</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>
        <span class="n">l_ptrs</span> <span class="o">=</span> <span class="n">L</span> <span class="o">+</span> <span class="n">off_hz</span> <span class="o">*</span> <span class="n">N_CTX</span> <span class="o">+</span> <span class="n">offs_m</span>
        <span class="c1"># Save LSE, converting from log2 to natural logarithm</span>
        <span class="n">l_mask</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">start_m</span> <span class="o">*</span> <span class="n">BLOCK_M</span> <span class="o">+</span> <span class="n">tl</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_M</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">q_len</span>
            <span class="k">if</span> <span class="n">BOUNDS_CHECKS_M</span>
            <span class="k">else</span> <span class="kc">None</span>
        <span class="p">)</span>
        <span class="n">tl</span><span class="o">.</span><span class="n">store</span><span class="p">(</span><span class="n">l_ptrs</span><span class="p">,</span> <span class="p">(</span><span class="n">m_i</span> <span class="o">+</span> <span class="n">tl</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">l_i</span><span class="p">))</span> <span class="o">/</span> <span class="mf">1.44269504</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">l_mask</span><span class="p">)</span>
        <span class="c1"># write back O</span>
        <span class="n">O_block_ptr</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">make_block_ptr</span><span class="p">(</span>
            <span class="n">base</span><span class="o">=</span><span class="n">Out</span> <span class="o">+</span> <span class="n">out_offset</span><span class="p">,</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">q_len</span><span class="p">,</span> <span class="n">BLOCK_DMODEL</span><span class="p">),</span>
            <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="n">stride_om</span><span class="p">,</span> <span class="n">stride_on</span><span class="p">),</span>
            <span class="n">offsets</span><span class="o">=</span><span class="p">(</span><span class="n">start_m</span> <span class="o">*</span> <span class="n">BLOCK_M</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
            <span class="n">block_shape</span><span class="o">=</span><span class="p">(</span><span class="n">BLOCK_M</span><span class="p">,</span> <span class="n">BLOCK_DMODEL</span><span class="p">),</span>
            <span class="n">order</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="n">tl</span><span class="o">.</span><span class="n">store</span><span class="p">(</span>
            <span class="n">O_block_ptr</span><span class="p">,</span>
            <span class="n">acc1</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">Out</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">element_ty</span><span class="p">),</span>
            <span class="n">boundary_check</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="k">if</span> <span class="n">BOUNDS_CHECKS_M</span> <span class="ow">or</span> <span class="n">USE_SEQ_POS_Q</span> <span class="k">else</span> <span class="p">(),</span>
        <span class="p">)</span>

    <span class="n">_autotuner_config_amd_full</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">triton</span><span class="o">.</span><span class="n">Config</span><span class="p">(</span>
            <span class="p">{</span><span class="s2">&quot;BLOCK_M&quot;</span><span class="p">:</span> <span class="mi">256</span><span class="p">,</span> <span class="s2">&quot;BLOCK_N&quot;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span> <span class="s2">&quot;waves_per_eu&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;pre_load_v&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">},</span>
            <span class="n">num_stages</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">num_warps</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="n">triton</span><span class="o">.</span><span class="n">Config</span><span class="p">(</span>
            <span class="p">{</span><span class="s2">&quot;BLOCK_M&quot;</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span> <span class="s2">&quot;BLOCK_N&quot;</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span> <span class="s2">&quot;waves_per_eu&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;pre_load_v&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">},</span>
            <span class="n">num_stages</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">num_warps</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="n">triton</span><span class="o">.</span><span class="n">Config</span><span class="p">(</span>
            <span class="p">{</span><span class="s2">&quot;BLOCK_M&quot;</span><span class="p">:</span> <span class="mi">256</span><span class="p">,</span> <span class="s2">&quot;BLOCK_N&quot;</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span> <span class="s2">&quot;waves_per_eu&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;pre_load_v&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">},</span>
            <span class="n">num_stages</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">num_warps</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="n">triton</span><span class="o">.</span><span class="n">Config</span><span class="p">(</span>
            <span class="p">{</span><span class="s2">&quot;BLOCK_M&quot;</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span> <span class="s2">&quot;BLOCK_N&quot;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span> <span class="s2">&quot;waves_per_eu&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="s2">&quot;pre_load_v&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">},</span>
            <span class="n">num_stages</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">num_warps</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
        <span class="p">),</span>  <span class="c1"># d64-False</span>
        <span class="n">triton</span><span class="o">.</span><span class="n">Config</span><span class="p">(</span>
            <span class="p">{</span><span class="s2">&quot;BLOCK_M&quot;</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span> <span class="s2">&quot;BLOCK_N&quot;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span> <span class="s2">&quot;waves_per_eu&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="s2">&quot;pre_load_v&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">},</span>
            <span class="n">num_stages</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">num_warps</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
        <span class="p">),</span>  <span class="c1"># d64-True</span>
    <span class="p">]</span>

    <span class="n">_autotuner_config_amd_dummy</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">triton</span><span class="o">.</span><span class="n">Config</span><span class="p">(</span>
            <span class="p">{</span><span class="s2">&quot;BLOCK_M&quot;</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span> <span class="s2">&quot;BLOCK_N&quot;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span> <span class="s2">&quot;waves_per_eu&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;pre_load_v&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">},</span>
            <span class="n">num_stages</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">num_warps</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
        <span class="p">),</span>
    <span class="p">]</span>

    <span class="n">_autotuner_config_nvidia_dummy</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">triton</span><span class="o">.</span><span class="n">Config</span><span class="p">(</span>
            <span class="p">{</span><span class="s2">&quot;BLOCK_M&quot;</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span> <span class="s2">&quot;BLOCK_N&quot;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span> <span class="s2">&quot;pre_load_v&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">},</span>
            <span class="n">num_stages</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">num_warps</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
        <span class="p">),</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">autotune_kernel</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="n">autotune</span><span class="p">):</span>

        <span class="n">kernel</span> <span class="o">=</span> <span class="n">triton</span><span class="o">.</span><span class="n">heuristics</span><span class="p">(</span>
            <span class="n">values</span><span class="o">=</span><span class="p">{</span>
                <span class="s2">&quot;BOUNDS_CHECKS_N&quot;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">args</span><span class="p">:</span> <span class="p">((</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;Mkv&quot;</span><span class="p">]</span> <span class="o">%</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;BLOCK_N&quot;</span><span class="p">])</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span>
                <span class="ow">or</span> <span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;USE_SEQ_POS_Q&quot;</span><span class="p">]</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;IS_KV_PADDED&quot;</span><span class="p">]),</span>
                <span class="s2">&quot;BOUNDS_CHECKS_M&quot;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">args</span><span class="p">:</span> <span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;N_CTX&quot;</span><span class="p">]</span> <span class="o">%</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;BLOCK_M&quot;</span><span class="p">])</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">,</span>
            <span class="p">}</span>
        <span class="p">)(</span><span class="n">kernel</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">cuda</span><span class="p">:</span>
            <span class="n">configs</span> <span class="o">=</span> <span class="n">_autotuner_config_nvidia_dummy</span>
        <span class="k">elif</span> <span class="n">autotune</span><span class="p">:</span>
            <span class="n">configs</span> <span class="o">=</span> <span class="n">_autotuner_config_amd_full</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">configs</span> <span class="o">=</span> <span class="n">_autotuner_config_amd_dummy</span>

        <span class="n">kernel</span> <span class="o">=</span> <span class="n">triton</span><span class="o">.</span><span class="n">autotune</span><span class="p">(</span>
            <span class="n">configs</span><span class="o">=</span><span class="n">configs</span><span class="p">,</span>
            <span class="n">key</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Z&quot;</span><span class="p">,</span> <span class="s2">&quot;H&quot;</span><span class="p">,</span> <span class="s2">&quot;N_CTX&quot;</span><span class="p">,</span> <span class="s2">&quot;IS_CAUSAL&quot;</span><span class="p">,</span> <span class="s2">&quot;BLOCK_DMODEL&quot;</span><span class="p">],</span>
        <span class="p">)(</span><span class="n">kernel</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">kernel</span>

    <span class="n">_fwd_kernel_triton_flash_maybe_autotuned</span> <span class="o">=</span> <span class="p">{</span>
        <span class="kc">True</span><span class="p">:</span> <span class="n">autotune_kernel</span><span class="p">(</span><span class="n">_fwd_kernel_triton_flash</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
        <span class="kc">False</span><span class="p">:</span> <span class="n">autotune_kernel</span><span class="p">(</span><span class="n">_fwd_kernel_triton_flash</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
    <span class="p">}</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">_fwd_kernel_triton_flash</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">_fwd_kernel_triton_flash_maybe_autotuned</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">_prepare_inputs</span><span class="p">(</span><span class="n">inp</span><span class="p">:</span> <span class="n">Inputs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Inputs</span><span class="p">:</span>
    <span class="n">attn_bias</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">attn_bias</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">attn_bias</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">attn_bias</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
        <span class="n">B</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">attn_bias</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="n">B</span>
        <span class="n">attn_bias</span> <span class="o">=</span> <span class="n">attn_bias</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">attn_bias</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">attn_bias</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>

    <span class="c1"># Make sure that the last dimension is contiguous</span>
    <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">x</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">x</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">[</span><span class="n">inp</span><span class="o">.</span><span class="n">query</span><span class="p">,</span> <span class="n">inp</span><span class="o">.</span><span class="n">key</span><span class="p">,</span> <span class="n">inp</span><span class="o">.</span><span class="n">value</span><span class="p">]</span>
    <span class="p">]</span>
    <span class="k">return</span> <span class="n">replace</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">attn_bias</span><span class="o">=</span><span class="n">attn_bias</span><span class="p">,</span> <span class="n">query</span><span class="o">=</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">value</span><span class="p">)</span>


<div class="viewcode-block" id="FwOp"><a class="viewcode-back" href="../../../../components/ops.html#xformers.ops.fmha.triton.FwOp">[docs]</a><span class="nd">@register_operator</span>
<span class="k">class</span> <span class="nc">FwOp</span><span class="p">(</span><span class="n">AttentionFwOpBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Operator that computes memory-efficient attention using \</span>
<span class="sd">        `Tri Dao&#39;s &lt;https://github.com/HazyResearch/flash-attention/blob/main/flash_attn/flash_attn_triton.py&gt;`_ \</span>
<span class="sd">        implementation, based on</span>
<span class="sd">        `Phil Tillet&#39;s code &lt;https://github.com/openai/triton/blob/master/python/tutorials/06-fused-attention.py&gt;`_</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">OPERATOR</span> <span class="o">=</span> <span class="n">_fwd_kernel_triton_flash</span>
    <span class="n">SUPPORTED_DEVICES</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;cuda&quot;</span><span class="p">}</span>
    <span class="n">CUDA_MINIMUM_COMPUTE_CAPABILITY</span> <span class="o">=</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">SUPPORTED_DTYPES</span> <span class="o">=</span> <span class="p">{</span><span class="n">torch</span><span class="o">.</span><span class="n">half</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">}</span>
    <span class="n">SUPPORTED_MAX_K</span> <span class="o">=</span> <span class="mi">128</span>
    <span class="n">SUPPORTED_ATTN_BIAS_TYPES</span><span class="p">:</span> <span class="n">Set</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
        <span class="nb">type</span><span class="p">(</span><span class="kc">None</span><span class="p">),</span>
        <span class="n">LowerTriangularMask</span><span class="p">,</span>
        <span class="n">BlockDiagonalCausalMask</span><span class="p">,</span>
        <span class="n">BlockDiagonalCausalWithOffsetPaddedKeysMask</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="n">SUPPORTS_DROPOUT</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">SUPPORTS_CUSTOM_SCALE</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">NAME</span> <span class="o">=</span> <span class="s2">&quot;tritonflashattF&quot;</span>

    <span class="c1"># Off by default to avoid slowing down tests.</span>
    <span class="c1"># Needs to be turned on explicitly in benchmarks, in prod, and in a small number of tests</span>
    <span class="n">AUTOTUNE</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="n">ERROR_ATOL</span><span class="p">:</span> <span class="n">Mapping</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">half</span><span class="p">:</span> <span class="mf">2e-2</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">:</span> <span class="mf">2e-2</span><span class="p">,</span>
    <span class="p">}</span>

    <span class="n">ERROR_RTOL</span><span class="p">:</span> <span class="n">Mapping</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">half</span><span class="p">:</span> <span class="mf">2e-2</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">:</span> <span class="mf">2e-2</span><span class="p">,</span>
    <span class="p">}</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">shape_not_supported_reasons</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span> <span class="n">Mq</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">Mkv</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">K</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">Kv</span><span class="p">:</span> <span class="nb">int</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
        <span class="n">reasons</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">shape_not_supported_reasons</span><span class="p">(</span><span class="n">Mq</span><span class="p">,</span> <span class="n">Mkv</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">Kv</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">K</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">{</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">}:</span>
            <span class="n">reasons</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Embed dim </span><span class="si">{</span><span class="n">K</span><span class="si">}</span><span class="s2"> not supported&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">reasons</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">not_supported_reasons</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">d</span><span class="p">:</span> <span class="n">Inputs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
        <span class="n">reasons</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">FwOp</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">not_supported_reasons</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
        <span class="n">check_lastdim_alignment_stride1</span><span class="p">(</span><span class="n">reasons</span><span class="p">,</span> <span class="s2">&quot;query&quot;</span><span class="p">,</span> <span class="n">d</span><span class="o">.</span><span class="n">query</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
        <span class="n">check_lastdim_alignment_stride1</span><span class="p">(</span><span class="n">reasons</span><span class="p">,</span> <span class="s2">&quot;key&quot;</span><span class="p">,</span> <span class="n">d</span><span class="o">.</span><span class="n">key</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
        <span class="n">check_lastdim_alignment_stride1</span><span class="p">(</span><span class="n">reasons</span><span class="p">,</span> <span class="s2">&quot;value&quot;</span><span class="p">,</span> <span class="n">d</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="n">d</span><span class="o">.</span><span class="n">attn_bias</span><span class="p">,</span>
            <span class="n">BlockDiagonalCausalWithOffsetPaddedKeysMask</span><span class="p">,</span>
        <span class="p">):</span>
            <span class="c1"># Support padded causal block-diagonal mask if the distance between each two consecutive key starts</span>
            <span class="c1"># is equal to the padding (key lengths can vary)</span>
            <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">d</span><span class="o">.</span><span class="n">attn_bias</span><span class="o">.</span><span class="n">q_seqinfo</span><span class="o">.</span><span class="n">seqstart_py</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
            <span class="n">B_T</span> <span class="o">=</span> <span class="n">d</span><span class="o">.</span><span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span>
                <span class="mi">1</span>
            <span class="p">]</span>  <span class="c1"># For these mask types the shapes of Q/K/V are (1, B_T, H, K)</span>
            <span class="k">if</span> <span class="n">B_T</span> <span class="o">%</span> <span class="n">batch_size</span><span class="p">:</span>
                <span class="n">reasons</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;K/V should be padded, but batch size </span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s2"> doesn&#39;t divide B*T=</span><span class="si">{</span><span class="n">B_T</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">kv_maxlen</span> <span class="o">=</span> <span class="n">d</span><span class="o">.</span><span class="n">attn_bias</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">padding</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">seqstart</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">d</span><span class="o">.</span><span class="n">attn_bias</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">seqstart_py</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">seqstart</span> <span class="o">!=</span> <span class="n">i</span> <span class="o">*</span> <span class="n">kv_maxlen</span><span class="p">:</span>
                        <span class="n">reasons</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                            <span class="s2">&quot;Variable K/V start positions are not supported, they should be determined &quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;by kv_maxlen/padding: </span><span class="si">{</span><span class="n">d</span><span class="o">.</span><span class="n">attn_bias</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">seqstart_py</span><span class="si">=}</span><span class="s2"> </span><span class="si">{</span><span class="n">kv_maxlen</span><span class="si">=}</span><span class="s2"> </span><span class="si">{</span><span class="n">batch_size</span><span class="si">=}</span><span class="s2">&quot;</span>
                        <span class="p">)</span>
                        <span class="k">break</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="n">d</span><span class="o">.</span><span class="n">attn_bias</span><span class="p">,</span>
            <span class="n">BlockDiagonalCausalMask</span><span class="p">,</span>
        <span class="p">):</span>
            <span class="c1"># Support padded causal block-diagonal mask if for each batch element number of queries is equal</span>
            <span class="c1"># to the number of key/values, i.e. each block is square</span>
            <span class="k">for</span> <span class="n">q_pos</span><span class="p">,</span> <span class="n">kv_pos</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
                <span class="n">d</span><span class="o">.</span><span class="n">attn_bias</span><span class="o">.</span><span class="n">q_seqinfo</span><span class="o">.</span><span class="n">seqstart_py</span><span class="p">,</span> <span class="n">d</span><span class="o">.</span><span class="n">attn_bias</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">seqstart_py</span>
            <span class="p">):</span>
                <span class="k">if</span> <span class="n">q_pos</span> <span class="o">!=</span> <span class="n">kv_pos</span><span class="p">:</span>
                    <span class="n">reasons</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Position starts of Q and K/V should be the same, but got </span><span class="si">{</span><span class="n">q_pos</span><span class="si">}</span><span class="s2"> != </span><span class="si">{</span><span class="n">kv_pos</span><span class="si">}</span><span class="s2">&quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">d</span><span class="o">.</span><span class="n">attn_bias</span><span class="o">.</span><span class="n">q_seqinfo</span><span class="o">.</span><span class="n">seqstart_py</span><span class="si">=}</span><span class="s2">, </span><span class="si">{</span><span class="n">d</span><span class="o">.</span><span class="n">attn_bias</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">seqstart_py</span><span class="si">=}</span><span class="s2">&quot;</span>
                    <span class="p">)</span>

        <span class="k">if</span> <span class="n">d</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">cuda</span><span class="p">:</span>
            <span class="c1"># Has only been tested on 8.0 / 9.0.</span>
            <span class="c1"># Fails on 7.5 with illegal memory access</span>
            <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_capability</span><span class="p">(</span><span class="n">d</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="o">&lt;</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
                <span class="n">reasons</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="s2">&quot;requires GPU with sm80 minimum compute capacity, e.g., A100/H100/L4&quot;</span>
                <span class="p">)</span>
        <span class="k">return</span> <span class="n">reasons</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">apply</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span> <span class="n">inp</span><span class="p">:</span> <span class="n">Inputs</span><span class="p">,</span> <span class="n">needs_gradient</span><span class="p">:</span> <span class="nb">bool</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Context</span><span class="p">]]:</span>
        <span class="n">inp</span> <span class="o">=</span> <span class="n">_prepare_inputs</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span>
        <span class="n">attn_bias</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">attn_bias</span>
        <span class="n">seq_len_kv</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">seqstart_q</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="n">q</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">query</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">key</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">value</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="n">attn_bias</span><span class="p">,</span>
            <span class="p">(</span><span class="n">BlockDiagonalCausalWithOffsetPaddedKeysMask</span><span class="p">,</span> <span class="n">BlockDiagonalCausalMask</span><span class="p">),</span>
        <span class="p">):</span>
            <span class="c1"># q ~ [1, B*T, H, K]</span>
            <span class="c1"># TODO: do we really need to do this cast? seems fishy but</span>
            <span class="c1"># I just copied it from the split-k kernel</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span>
                <span class="n">attn_bias</span><span class="p">,</span>
                <span class="p">(</span><span class="n">BlockDiagonalCausalWithOffsetPaddedKeysMask</span><span class="p">,</span> <span class="n">BlockDiagonalCausalMask</span><span class="p">),</span>
            <span class="p">)</span>
            <span class="n">attn_bias</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">query</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">attn_bias</span><span class="o">.</span><span class="n">q_seqinfo</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">query</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">seqstart_q</span> <span class="o">=</span> <span class="n">attn_bias</span><span class="o">.</span><span class="n">q_seqinfo</span><span class="o">.</span><span class="n">seqstart</span>
            <span class="n">B</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">seqstart_q</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
            <span class="n">H</span><span class="p">,</span> <span class="n">Kq</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span>
            <span class="n">H2</span><span class="p">,</span> <span class="n">Kkv</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span>

            <span class="n">Mq</span> <span class="o">=</span> <span class="n">attn_bias</span><span class="o">.</span><span class="n">q_seqinfo</span><span class="o">.</span><span class="n">max_seqlen</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">attn_bias</span><span class="p">,</span> <span class="n">BlockDiagonalCausalWithOffsetPaddedKeysMask</span><span class="p">):</span>
                <span class="n">seq_len_kv</span> <span class="o">=</span> <span class="n">attn_bias</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">seqlen</span>
                <span class="c1"># assume kv has been padded</span>
                <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">H2</span><span class="p">,</span> <span class="n">Kkv</span><span class="p">)</span>
                <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">H2</span><span class="p">,</span> <span class="n">Kkv</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">B</span><span class="p">,</span> <span class="n">Mq</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># Coded for BHMK format</span>
        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">q</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
            <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
            <span class="n">v</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>

        <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">Mkv</span><span class="p">,</span> <span class="n">K</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">shape</span>

        <span class="n">sm_scale</span> <span class="o">=</span> <span class="n">K</span><span class="o">**-</span><span class="mf">0.5</span> <span class="k">if</span> <span class="n">inp</span><span class="o">.</span><span class="n">scale</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">inp</span><span class="o">.</span><span class="n">scale</span>
        <span class="n">L</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">B</span> <span class="o">*</span> <span class="n">H</span><span class="p">,</span> <span class="n">Mq</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">q</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">is_causal</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">attn_bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="n">use_seq_len_kv</span> <span class="o">=</span> <span class="n">seq_len_kv</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="n">use_seq_pos_q</span> <span class="o">=</span> <span class="n">seqstart_q</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="n">is_kv_padded</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="n">attn_bias</span><span class="p">,</span> <span class="n">BlockDiagonalCausalWithOffsetPaddedKeysMask</span>
        <span class="p">)</span>

        <span class="n">grid</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">META</span><span class="p">:</span> <span class="p">(</span><span class="n">triton</span><span class="o">.</span><span class="n">cdiv</span><span class="p">(</span><span class="n">Mq</span><span class="p">,</span> <span class="n">META</span><span class="p">[</span><span class="s2">&quot;BLOCK_M&quot;</span><span class="p">]),</span> <span class="n">B</span> <span class="o">*</span> <span class="n">H</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># noqa: E731</span>
        <span class="n">kernel</span> <span class="o">=</span> <span class="n">_fwd_kernel_triton_flash_maybe_autotuned</span><span class="p">[</span><span class="bp">cls</span><span class="o">.</span><span class="n">AUTOTUNE</span><span class="p">]</span>
        <span class="n">kernel</span><span class="p">[</span><span class="n">grid</span><span class="p">](</span>
            <span class="n">q</span><span class="p">,</span>
            <span class="n">k</span><span class="p">,</span>
            <span class="n">v</span><span class="p">,</span>
            <span class="n">sm_scale</span><span class="p">,</span>
            <span class="n">L</span><span class="p">,</span>
            <span class="n">out</span><span class="p">,</span>
            <span class="n">seq_len_kv</span><span class="p">,</span>
            <span class="n">seqstart_q</span><span class="p">,</span>
            <span class="n">q</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
            <span class="n">q</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">q</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>
            <span class="n">q</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
            <span class="n">k</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
            <span class="n">k</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">k</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>
            <span class="n">k</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
            <span class="n">v</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
            <span class="n">v</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">v</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>
            <span class="n">v</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
            <span class="n">out</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
            <span class="n">out</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">out</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>
            <span class="n">out</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
            <span class="n">B</span><span class="p">,</span>
            <span class="n">H</span><span class="p">,</span>
            <span class="n">Mq</span><span class="p">,</span>
            <span class="n">Mkv</span><span class="p">,</span>
            <span class="n">BLOCK_DMODEL</span><span class="o">=</span><span class="n">K</span><span class="p">,</span>
            <span class="n">IS_CAUSAL</span><span class="o">=</span><span class="n">is_causal</span><span class="p">,</span>
            <span class="n">USE_SEQ_LEN_KV</span><span class="o">=</span><span class="n">use_seq_len_kv</span><span class="p">,</span>
            <span class="n">USE_SEQ_POS_Q</span><span class="o">=</span><span class="n">use_seq_pos_q</span><span class="p">,</span>
            <span class="n">IS_KV_PADDED</span><span class="o">=</span><span class="n">is_kv_padded</span><span class="p">,</span>
            <span class="n">ALLOW_TF32</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">matmul</span><span class="o">.</span><span class="n">allow_tf32</span><span class="p">,</span>
            <span class="n">CAST_BEFORE_MATMUL</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">L</span> <span class="o">=</span> <span class="n">L</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">Mq</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">Context</span><span class="p">(</span><span class="n">lse</span><span class="o">=</span><span class="n">L</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">out</span><span class="p">)</span></div>
</pre></div>

              </article>
              
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright Copyright © 2021 Meta Platforms, Inc.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <!-- <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div> -->
    </section>
  </div>

  


  

  
  <script type="text/javascript" id="documentation_options" data-url_root="../../../../"
    src="../../../../_static/documentation_options.js"></script>
  <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
  <script src="../../../../_static/jquery.js"></script>
  <script src="../../../../_static/underscore.js"></script>
  <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
  <script src="../../../../_static/doctools.js"></script>
  

  

  <script type="text/javascript" src="../../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->


  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://github.com/facebookresearch/xformers" class="footer-logo"></a>
      </div>
      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://github.com/facebookresearch/xformers">fairscale</a></li>
            <li><a href="https://github.com/facebookresearch/xformers/blob/master/README.md">Get Started</a></li>
            <li><a href="https://github.com/facebookresearch/xformers/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="">Resources</a></li>
            <li><a href="https://github.com/facebookresearch/xformers">Docs</a></li>
            <li><a href="https://github.com/facebookresearch/xformers/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://opensource.facebook.com/legal/terms">Terms of Use</a></li>
            <li><a href="https://opensource.facebook.com/legal/privacy">Privacy Policy</a></li>
          </ul>
        </div>
      </div>
    </div>
    </div>
  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://github.com/facebookresearch/xformers" aria-label="fairscale"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/facebookresearch/xformers/blob/master/README.md">Get Started</a>
          </li>

          <li>
            <a href="https://github.com/facebookresearch/xformers">Docs</a>
          </li>

          <li>
            <a href="https://github.com/facebookresearch/xformers">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function () {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function (e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>

</html>